# -*- coding: utf-8 -*-
"""Cópia de Verdefica - Limpeza dos Dados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XmYyg-biJOyFm4B-Ej3oeui98oRpu-aI

**1. Preparação**

Importação das bibliotecas
"""

import pandas as pd
import matplotlib.pyplot as plt
import unicodedata
import numpy as np
from pathlib import Path

"""Importação do dataset (censo_arboreo_final.csv)"""

# Caminho do arquivo - ajuste conforme necessário
import os

# Tenta encontrar o arquivo em diferentes locais
base_dir = Path(__file__).parent
caminho = None

# Possíveis locais do arquivo
possiveis_caminhos = [
    base_dir / "censo_arboreo_final.csv",
    base_dir / "Dataset" / "censo_arboreo_final.csv",
    base_dir.parent / "Dataset" / "censo_arboreo_final.csv",
]

for caminho_teste in possiveis_caminhos:
    if caminho_teste.exists():
        caminho = caminho_teste
        break

if caminho is None:
    # Se não encontrar, pede para o usuário especificar
    print("Arquivo 'censo_arboreo_final.csv' não encontrado.")
    print("Por favor, coloque o arquivo no mesmo diretório do script ou ajuste o caminho abaixo.")
    caminho = base_dir / "censo_arboreo_final.csv"  # Tenta usar este caminho mesmo assim

if not caminho.exists():
    raise FileNotFoundError(
        f"Arquivo não encontrado: {caminho}\n"
        f"Por favor, coloque o arquivo 'censo_arboreo_final.csv' no diretório: {base_dir}"
    )

print(f"Lendo arquivo: {caminho}")
df = pd.read_csv(caminho, encoding='utf-8', low_memory=False)

df.info()

"""A partir do dataset disponibilizado pela Prefeitura no Portal Dados Abertos, será realizada uma etapa de limpeza e normalização dos dados, essencial para garantir a qualidade e consistência das análises subsequentes. O conjunto de dados possui formato tabular, com colunas delimitadas por vírgula, em que cada linha representa uma árvore registrada no censo e identificada de forma única pela coluna globalid, mantida como chave principal do dataset.

**2. Análise de Integridade**

Registros nulos
"""

# Percentual de valores nulos em todas as colunas
percent_nulos = (df.isna().mean() * 100).sort_values(ascending=False).round(2)

# Mostrar todas as colunas com seus percentuais
pd.DataFrame(percent_nulos, columns=["% de nulos"])

# Lista final de colunas que devem ser mantidas
colunas_mantidas = [
    'responsave',
    'datamonito',     # data de monitoramento
    'nomecienti',     # nome científico
    'projeto',        # programa/campanha de arborização
    'tipologia',      # tipo de ponto de plantio
    'dataplanti',     # data de plantio
    'nome_popul',     # nome popular
    'injuria',        # ocorrência de injúrias
    'fitossanid',     # estado fitossanitário
    'porte_esp',      # porte da espécie
    'objectid',
    'bairro',
    'rpa',
    'x',
    'y',
    'categoria_',
    'obs',
    'x_wgs84',
    'y_wgs84',
    'altura',
    'dap',
    'copa',
    'cap',
    'nome_comum',
    'longitude',
    'latitude',
    'id',
    'globalid'
]

# Filtra o DataFrame para manter apenas as colunas da lista (ignorando as que não existirem)
df = df.loc[:, df.columns.intersection(colunas_mantidas)]

# Percentual de valores nulos em todas as colunas
percent_nulos = (df.isna().mean() * 100).sort_values(ascending=False).round(2)

# Mostrar todas as colunas com seus percentuais
pd.DataFrame(percent_nulos, columns=["% de nulos"])

"""Renomeando colunas"""

# Dicionário de renomeação das colunas
df = df.rename(columns={
    'responsave': 'responsavel',
    'datamonito': 'data_monitoramento',
    'nomecienti': 'nome_cientifico',
    'projeto': 'projeto',
    'tipologia': 'tipologia',
    'dataplanti': 'data_plantio',
    'nome_popul': 'nome_popular',
    'injuria': 'injuria',
    'fitossanid': 'fitossanid',
    'porte_esp': 'porte_especie',
    'objectid': 'objectid',
    'bairro': 'bairro',
    'rpa': 'rpa',
    'x': 'x',
    'y': 'y',
    'categoria_': 'categoria',
    'obs': 'observacao',
    'x_wgs84': 'x_wgs84',
    'y_wgs84': 'y_wgs84',
    'altura': 'altura',
    'dap': 'dap',
    'copa': 'copa',
    'cap': 'cap',
    'nome_comum': 'nome_comum',
    'longitude': 'longitude',
    'latitude': 'latitude',
    'id': 'id',
    'globalid': 'globalid'
})

# Exibe o resultado
print("Colunas renomeadas com sucesso:")
print(df.columns.tolist())

df.info()

"""Registros duplicados (a partir da globalid)"""

# Verifica se há registros duplicados na coluna 'globalid'
duplicados = df[df.duplicated(subset='globalid', keep=False)]

# Mostra os registros duplicados (se existirem)
print(f"Total de registros duplicados em 'globalid': {duplicados.shape[0]}")
duplicados.head()

"""Corrigindo dados com RPA 0"""

# Listar siglas únicas na RPA 0
siglas_rpa0 = (
    df[df['rpa'] == 0]['bairro']
      .dropna()
      .unique()
)
siglas_rpa0 = sorted(siglas_rpa0)
print(f"Siglas na RPA 0 ({len(siglas_rpa0)}):\n{siglas_rpa0}")

# Corrigir RPA 0 com base na RPA mais frequente de cada sigla
rpa_por_sigla = (
    df[df['rpa'] != 0]
      .groupby('bairro')['rpa']
      .agg(lambda x: x.mode().iat[0])  # pega a RPA mais comum
      .to_dict()
)

# Atualizar registros com rpa = 0
df.loc[df['rpa'] == 0, 'rpa'] = df.loc[df['rpa'] == 0, 'bairro'].map(rpa_por_sigla)

# Conferir se ainda restam RPAs 0
print("Registros restantes com RPA 0:", df[df['rpa'] == 0].shape[0])

"""**3. Padronização da grafia**

Padronização da grafia das espécies
"""

# ============================================
# CORREÇÃO FINAL DE GRAFIAS – Censo Arbóreo
# (coluna: nome_popular_padrao)
# ============================================

import re, unicodedata, pandas as pd

# --- segurança: a coluna alvo precisa existir ---
# Verificar se a coluna já existe (dados já limpos) ou precisa ser criada
if "nome_popular_padrao" not in df.columns:
    if "nome_popular" in df.columns:
        df["nome_popular_padrao"] = df["nome_popular"].copy()
    else:
        # Se não houver nenhuma das colunas, criar uma vazia
        df["nome_popular_padrao"] = None
        print("Aviso: Coluna 'nome_popular' nao encontrada. Criando coluna vazia.")
else:
    # Se já existe, garantir que não está vazia e preencher com nome_popular se necessário
    if df["nome_popular_padrao"].isna().all() and "nome_popular" in df.columns:
        df["nome_popular_padrao"] = df["nome_popular"].copy()
        print("Coluna 'nome_popular_padrao' estava vazia, preenchida com 'nome_popular'.")

# --- helper: chave para checar duplicidades (ignora acento, hífen e caixa) ---
def _key_equiv(s):
    if pd.isna(s): return s
    s = unicodedata.normalize("NFKD", str(s)).encode("ascii","ignore").decode("utf-8")
    return re.sub(r"[-\s]+", " ", s.strip().lower())

# --- dicionário de correções finais (consolidado) ---
final_fix = {
    # ====== Ipês ======
    "Ipeamarelo": "Ipê-Amarelo",
    "Ipê-Amarelo (gp)": "Ipê-Amarelo",
    "Ipê-amarelo": "Ipê-Amarelo",
    "Ipê amarelo": "Ipê-Amarelo",
    "ipê amarelo": "Ipê-Amarelo",
    "ipê-roxo": "Ipê-Roxo",
    "Ipê-roxo": "Ipê-Roxo",
    "Ipê roxo": "Ipê-Roxo",
    "Ipê Roxo": "Ipê-Roxo",
    "Ipê-branco": "Ipê-Branco",
    "Ipê branco": "Ipê-Branco",
    "Ipê rosa": "Ipê-Rosa",
    "Ipê-rosa": "Ipê-Rosa",
    "Ipê-Branco-da-Restinga": "Ipê-Branco-da-Restinga",

    # ====== Jasmins / Jasmim-manga ======
    # pedido: agrupar "(jasmim-Vapor)" em "Jasmim Vapor"
    "Jasmim-Manga (jasmim-Vapor)": "Jasmim Vapor",
    "Jasmim-Manga (Jasmim-Vapor)": "Jasmim Vapor",
    # unificar hífen vs espaço
    "Jasmim Vapor": "Jasmim-Vapor",
    # correções específicas
    "Jasmin Manga Branca": "Jasmim-Manga-Branca",
    "Jasmim Manga Branca": "Jasmim-Manga-Branca",
    "Jasmim-Manga-Branca": "Jasmim-Manga-Branca",
    "Jasmim Manga Rosa": "Jasmim-Manga-Rosa",
    # padronizar para "Jasmin Branco"
    "Jasmim-Branco": "Jasmin Branco",
    "Jasmim Branco": "Jasmin Branco",
    "Jasmim-branco": "Jasmin Branco",

    # ====== Pau-de-Jangada ======
    "Pau Jangada": "Pau-Jangada",
    "pau jangada": "Pau-Jangada",

    # ====== Pau-Ferro ======
    "Pau Ferro": "Pau-Ferro",
    "Pau Ferro da Mata": "Pau-Ferro-da-Mata",

    # ====== Sibipiruna ======
    "Sibipíruna": "Sibipiruna",

     # Pau-Brasil
    "Pau Brasil": "Pau-Brasil",
    "Pau brasil": "Pau-Brasil",
    "Pau-brasil": "Pau-Brasil",
    "pau brasil": "Pau-Brasil",
    "Pau-brasil'": "Pau-Brasil",   # caso com apóstrofo perdido
    "Pau-brasíl": "Pau-Brasil",

    # Palmeira-Imperial
    "Palmeira Imperial": "Palmeira-Imperial",
    "palmeira imperial": "Palmeira-Imperial",
    "Palmeira imperial": "Palmeira-Imperial",
    "palmeira-imperial": "Palmeira-Imperial",
    "Palmeira-imperial": "Palmeira-Imperial",

    # Algodão-da-Praia
    "Algodao da praia": "Algodão-da-Praia",
    "Algodao-da-praia": "Algodão-da-Praia",
    "algodao da praia": "Algodão-da-Praia",
    "Algodão da praia": "Algodão-da-Praia",
    "Algodão-da-praia": "Algodão-da-Praia",

    # Ipê-Rosa (alguns já cobertos, reforçando variações)
    "Ipe rosa": "Ipê-Rosa",
    "ipe rosa": "Ipê-Rosa",
    "Ipê Rosa": "Ipê-Rosa",
    "Ipê-rosa": "Ipê-Rosa",

    # Acácia Amarela
    "acacia amarela": "Acácia Amarela",
    "Acacia Amarela": "Acácia Amarela",
    "Acacia amarela": "Acácia Amarela",
    "Acácia amarela": "Acácia Amarela",

    # Amescla de Cheiro
    "Amescla de cheiro": "Amescla de Cheiro",
    "Amescla-de-cheiro": "Amescla de Cheiro",
    "amescla de cheiro": "Amescla de Cheiro",

    # Jasmim Buquê de Noiva
    "jasmim buque de noiva": "Jasmim Buquê de Noiva",
    "Jasmim buque de noiva": "Jasmim Buquê de Noiva",
    "Jasmim buquê de noiva": "Jasmim Buquê de Noiva",

    # Jasmim-Laranja
    "jasmim laranja": "Jasmim-Laranja",
    "Jasmim laranja": "Jasmim-Laranja",
    "Jasmim-laranja": "Jasmim-Laranja",

    # Olho-de-Pombo
    "olho de pombo": "Olho-de-Pombo",
    "Olho de Pombo": "Olho-de-Pombo",
    "olho-de-pombo": "Olho-de-Pombo",
    "Olho-de-pombo": "Olho-de-Pombo",

    # Pau-Formiga
    "Pau formiga": "Pau-Formiga",
    "pau formiga": "Pau-Formiga",
    "Pau-formiga": "Pau-Formiga",
    "Pau Formiga": "Pau-Formiga",


    # Pau-Jangada (reforço)
    "Pau jangada": "Pau-Jangada",
    "pau jangada": "Pau-Jangada",
    "Pau-jangada": "Pau-Jangada",
}

# --- aplicar correções ---
df["nome_popular_padrao"] = (
    df["nome_popular_padrao"]
      .replace(final_fix)
      .str.replace(r"\s+", " ", regex=True)   # normaliza espaços
      .str.strip()
)

# --- diagnóstico rápido ---
print("✅ Correções aplicadas com sucesso!\n")
print(f"Total de espécies únicas: {df['nome_popular_padrao'].nunique()}")

key_final = df["nome_popular_padrao"].map(_key_equiv)
suspeitos = df.groupby(key_final)["nome_popular_padrao"].nunique().sort_values(ascending=False)
suspeitos = suspeitos[suspeitos > 1]
if len(suspeitos):
    print("\n⚠️ Possíveis duplicidades de grafia ainda existentes:")
    for k in suspeitos.index:
        formas = sorted(df.loc[key_final == k, "nome_popular_padrao"].unique())
        print(f"- '{k}' -> {formas}")
else:
    print("\nNenhuma duplicidade aparente. Tudo padronizado!")

# (opcional) ver as 20 primeiras em ordem alfabética
print("\nEspécies:")
for especie in sorted(df["nome_popular_padrao"].dropna().unique()):
    print(especie)

"""Correção da grafia e agrupamento nas colunas fitossanid e injuria"""

# ============================================
# AGRUPAMENTO DE CATEGORIAS — fitossanid & injuria
# cria colunas: fitossanid_grupo, injuria_grupo
# ============================================
import re, unicodedata, pandas as pd

# ---------- helpers ----------
def norm_txt(s):
    """minúsculo, sem acento, espaços normalizados (para matching)."""
    if pd.isna(s):
        return s
    s = unicodedata.normalize("NFKD", str(s)).encode("ascii", "ignore").decode("utf-8")
    s = re.sub(r"\s+", " ", s.strip().lower())
    return s

def contains_any(texto_norm, patterns):
    return any(re.search(p, texto_norm or "") for p in patterns)

# ---------- FITOSSANID: 4 grupos ----------
# exemplos sujos: 'saudavel', 'pragas', 'folhasamareladas', 'ruim', etc.
P_SAUDAVEL   = [r"\bsaud", r"\bsadio\b", r"\bboa?\b", r"\bok\b", r"\bnormal\b"]
P_INJURIADA  = [
    r"\binjur", r"\bdoenc", r"\bprag", r"\bfung", r"\bbicho\b", r"\bcupim",
    r"\bmancha", r"\bcloros", r"\bamarel", r"\bse(c|ç)a\b", r"\bseca de ponteiro",
    r"\bdanif", r"\bles(ao|aoes)", r"\bnecros", r"\bapodrec"
]
P_MORTA      = [r"\bmort", r"\bcepa\b", r"\btoco\b", r"\bseca total"]
P_NAOAVAL    = [r"\bnao aval", r"\bnao informado", r"\bsem info", r"\bsem aval", r"\bn/?a\b"]

def class_fitossanid(x):
    xn = norm_txt(x)
    if pd.isna(x) or contains_any(xn, P_NAOAVAL) or xn in ("",):
        return "Não avaliada"
    if contains_any(xn, P_MORTA):     return "Morta"
    if contains_any(xn, P_SAUDAVEL):  return "Saudável"
    if contains_any(xn, P_INJURIADA): return "Injuriada"
    # fallback (casos residuais vão para Não avaliada, mas listamos no relatório)
    return "Não avaliada"

# ---------- INJURIA: 3 grupos + "Não informada" ----------
# exemplos citados: poda drástica, ferida, rebaixamento de copa, deformação de copa
P_ANTROPICA  = [
    r"\bpoda", r"\brebaixamento de copa", r"\brebaixar copa", r"\bdesbaste",
    r"\bvandal", r"\bfixa(c|ç)ao de placa", r"\bplaca", r"\bpichac",
    r"\bchoque\b|\bbatida|\batropelamento", r"\bcorte de raiz", r"\bescarvac?ao",
    r"\bfiação|\bfio|\bred(e|es) eletric", r"\bconstruc?ao", r"\bobra(s)?"
]
P_MECANICA   = [
    r"\bferid", r"\bescori", r"\bfissur", r"\btrinc", r"\bquebr", r"\bles(a|ã)o mec",
    r"\barranh", r"\blascar", r"\bgalho quebrado"
]
P_ESTRUTFIS  = [
    r"\bdeformac?a?o de copa", r"\bformato irregular", r"\binclin", r"\boco\b|\bcavidad",
    r"\braiz(es)? expost", r"\btombament", r"\binstabil", r"\bapodrec"
]
P_NINFO      = [r"\bnao informado", r"\bnao se aplica", r"\bsem info", r"\bsem registro", r"\bn/?a\b"]

def class_injuria(x):
    xn = norm_txt(x)
    if pd.isna(x) or contains_any(xn, P_NINFO) or xn in ("",):
        return "Não informada"
    if contains_any(xn, P_ANTROPICA): return "Antrópica"
    if contains_any(xn, P_MECANICA):  return "Mecânica"
    if contains_any(xn, P_ESTRUTFIS): return "Estrutural/Fisiológica"
    # fallback — preferimos marcar como "Não informada" e sinalizar no relatório
    return "Não informada"

# ---------- aplicar no DataFrame (criando novas colunas) ----------
COL_FITO = "fitossanid"
COL_INJ  = "injuria"
assert COL_FITO in df.columns, f"Coluna '{COL_FITO}' não encontrada."
assert COL_INJ  in df.columns, f"Coluna '{COL_INJ}' não encontrada."

df["fitossanid_grupo"] = df[COL_FITO].map(class_fitossanid)
df["injuria_grupo"]    = df[COL_INJ].map(class_injuria)

# ---------- relatórios rápidos ----------
print("=== fitossanid_grupo ===")
print(df["fitossanid_grupo"].value_counts(dropna=False).to_string())

print("\n=== injuria_grupo ===")
print(df["injuria_grupo"].value_counts(dropna=False).to_string())

# Itens originais não mapeados claramente (úteis para revisão)
def listar_residuos(col_src, col_grp, alvo="Não avaliada"):
    base = df.loc[(df[col_grp] == alvo) & df[col_src].notna(), col_src].astype(str)
    out = (base.apply(norm_txt).value_counts().head(30))
    return out

print("\nPossíveis rótulos de FITOSSANID que caíram em 'Não avaliada' (top 30):")
print(listar_residuos(COL_FITO, "fitossanid_grupo", "Não avaliada"))

print("\nPossíveis rótulos de INJURIA que caíram em 'Não informada' (top 30):")
print(listar_residuos(COL_INJ, "injuria_grupo", "Não informada"))

# ---------- (opcional) salvar CSV ----------
# df.to_csv("censo_arboreo_categorizado.csv", index=False)

df.info()

print("\n=== fitossanid_grupo ===")
print(df["fitossanid_grupo"].value_counts(dropna=False))

print("\n=== injuria_grupo ===")
print(df["injuria_grupo"].value_counts(dropna=False))

# === Valores únicos de injuria ===
print("=== injuria (valores únicos) ===")
for v in sorted(df["injuria"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['injuria'].nunique()}")

# === Valores únicos de fitossanid ===
print("\n=== fitossanid (valores únicos) ===")
for v in sorted(df["fitossanid"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['fitossanid'].nunique()}")

# ============================================
# UNIFICAÇÃO DE GRAFIAS — fitossanid e injuria
# (primeira letra maiúscula + limpeza de duplicatas)
# ============================================

import re, unicodedata, pandas as pd

# --- função auxiliar para normalizar texto e deixar inicial maiúscula ---
def limpar_texto(s):
    if pd.isna(s):
        return s
    s = str(s).strip().lower()
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("utf-8")
    # corrige espaços e vírgulas
    s = re.sub(r"\s*,\s*", ", ", s)
    s = re.sub(r"\s+", " ", s)
    # corrige palavras coladas comuns
    s = s.replace("folhasamareladas", "folhas amareladas")
    s = s.replace("podainadequada", "poda inadequada")
    s = s.replace("podam", "poda")
    s = s.replace("poda drastica", "poda drástica")
    # capitaliza a primeira letra
    return s.capitalize()

# --- aplicar normalização ---
df["injuria_corrigida"] = df["injuria"].map(limpar_texto)
df["fitossanid_corrigida"] = df["fitossanid"].map(limpar_texto)

# --- exibir valores únicos após correção ---
print("=== injuria_corrigida (valores únicos) ===")
for v in sorted(df["injuria_corrigida"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['injuria_corrigida'].nunique()}")

print("\n=== fitossanid_corrigida (valores únicos) ===")
for v in sorted(df["fitossanid_corrigida"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['fitossanid_corrigida'].nunique()}")

# ============================================
# PADRONIZAÇÃO FINAL — sobrescrevendo as colunas
# injuria_corrigida  e  fitossanid_corrigida
# ============================================

import re, pandas as pd

# --- utilidades ---
def _clean_commas_spaces(s: str) -> str:
    s = re.sub(r"\s*,\s*", ", ", s)     # vírgula com um único espaço
    s = re.sub(r"\s+", " ", s)          # múltiplos espaços → 1
    s = s.strip(" ,")                   # remove vírgulas e espaços nas pontas
    return s.strip()

def _capitalize_first(s: str) -> str:
    return s[:1].upper() + s[1:] if s else s


# --- função para padronizar injuria ---
def padroniza_injuria(txt):
    if pd.isna(txt) or str(txt).strip() == "":
        return txt
    s = _clean_commas_spaces(str(txt).lower())
    s = s.replace("drastica", "drástica")  # corrige acento
    partes = [p.strip() for p in s.split(",") if p.strip()]
    base = partes[0]
    detalhes = partes[1:]
    if detalhes:
        s_final = f"{base} ({', '.join(detalhes)})"
    else:
        s_final = base
    s_final = s_final.replace("( ,", "(").replace(", )", ")")
    s_final = _clean_commas_spaces(s_final)
    return _capitalize_first(s_final)


# --- função para padronizar fitossanid ---
def padroniza_fitossanid(txt):
    if pd.isna(txt) or str(txt).strip() == "":
        return txt
    s = _clean_commas_spaces(str(txt).lower())
    m = re.match(r"^(presenca de cavidade\s*\([^)]+\))\s*,\s*(.+)$", s)
    if m:
        prefixo = m.group(1)
        sufixo  = _clean_commas_spaces(m.group(2))
        nivel = re.search(r"\(([^)]+)\)", prefixo).group(1)
        s = f"presenca de cavidade ({_clean_commas_spaces(nivel + ', ' + sufixo)})"
    s = _capitalize_first(s)
    return s


# --- aplicar diretamente nas colunas existentes ---
df["injuria_corrigida"] = df["injuria_corrigida"].map(padroniza_injuria)
df["fitossanid_corrigida"] = df["fitossanid_corrigida"].map(padroniza_fitossanid)

# --- verificação ---
print("=== injuria_corrigida (valores únicos) ===")
for v in sorted(df["injuria_corrigida"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['injuria_corrigida'].nunique()}")

print("\n=== fitossanid_corrigida (valores únicos) ===")
for v in sorted(df["fitossanid_corrigida"].dropna().unique()):
    print(v)
print(f"\nTotal de valores únicos: {df['fitossanid_corrigida'].nunique()}")

"""Mapeamento dos bairros - conversão das siglas na coluna bairro"""

# Dicionário de mapeamento: SIGLA → NOME COMPLETO

sigla_para_nome = {
    # RPA 1 - Centro
    'BVS': 'Boa Vista',
    'CBG': 'Cabanga',
    'COE': 'Coelhos',
    'IJB': 'Ilha Joana Bezerra',
    'ILT': 'Ilha do Leite',
    'PAI': 'Paissandu',
    'REC': 'Recife',
    'SAM': 'Santo Amaro',
    'SAT': 'Santo Antônio',
    'SJE': 'São José',
    'SLD': 'Soledade',

    # RPA 2 - Norte
    'AGF': 'Água Fria',
    'ARR': 'Arruda',
    'AST': 'Alto Santa Terezinha',
    'BBH': 'Bomba do Hemetério',
    'BEB': 'Beberibe',
    'CAJ': 'Cajueiro',
    'CDB': 'Campina do Barreto',
    'CMG': 'Campo Grande',
    'DUN': 'Dois Unidos',
    'ENC': 'Encruzilhada',
    'FUN': 'Fundão',
    'HIP': 'Hipódromo',
    'LTR': 'Linha do Tiro',
    'PEI': 'Peixinhos',
    'POP': 'Porto da Madeira',
    'PTM': 'Ponto de Parada',
    'RSA': 'Rosarinho',
    'TAO': 'Tamarineira',

    # RPA 3 - Noroeste
    'ADM': 'Alto do Mandu',
    'AFT': 'Aflitos',
    'AJP': 'Alto José do Pinho',
    'AJB': 'Alto José Bonifácio',
    'APB': 'Apipucos',
    'BBE': 'Brejo de Beberibe',
    'BGU': 'Brejo da Guabiraba',
    'CJE': 'Córrego do Jenipapo',
    'CSF': 'Casa Forte',
    'DBY': 'Derby',
    'DIR': 'Dois Irmãos',
    'ESP': 'Espinheiro',
    'GRA': 'Graças',
    'GUA': 'Guabiraba',
    'JAQ': 'Jaqueira',
    'MAC': 'Macaxeira',
    'MCO': 'Morro da Conceição',
    'MGA': 'Mangabeira',
    'MON': 'Monteiro',
    'NOV': 'Nova Descoberta',
    'PAR': 'Parnamirim',
    'PAS': 'Passarinho',
    'PAU': 'Pau Ferro',
    'POC': 'Poço',
    'SDP': 'Sítio dos Pintos',
    'STN': 'Santana',
    'TMA': 'Torreão',
    'VCG': 'Vasco da Gama',

    # RPA 4 - Oeste
    'CAX': 'Caxangá',
    'CDU': 'Cidade Universitária',
    'CRD': 'Cordeiro',
    'ENG': 'Engenho do Meio',
    'IPU': 'Iputinga',
    'IRE': 'Ilha do Retiro',
    'MAD': 'Madalena',
    'PRA': 'Prado',
    'TES': 'Torrões',
    'TRR': 'Torre',
    'VRZ': 'Várzea',
    'ZMB': 'Zumbi',

    # RPA 5 - Sudoeste
    'AFG': 'Afogados',
    'ARE': 'Areias',
    'BAR': 'Barro',
    'BNG': 'Bongi',
    'CCT': 'Cacote',
    'COQ': 'Coqueiral',
    'CUR': 'Curado',
    'EST': 'Estância',
    'JIQ': 'Jiquiá',
    'JSP': 'Jardim São Paulo',
    'MGE': 'Mangueira',
    'MUS': 'Mustardinha',
    'SMT': 'San Martin',
    'SNC': 'Sancho',
    'TEJ': 'Tejipió',
    'TOT': 'Totó',

    # RPA 6 - Sul
    'BRT': 'Brasília Teimosa',
    'BVG': 'Boa Viagem',
    'CHB': 'Cohab',
    'IBU': 'Ibura',
    'IMB': 'Imbiribeira',
    'IPS': 'Ipsep',
    'JOR': 'Jordão',
    'PIN': 'Pina'
}


# Criar nova coluna com o nome completo do bairro
df['bairro_nome'] = df['bairro'].map(sigla_para_nome)

# Verificar se há siglas não mapeadas
faltantes = df[df['bairro_nome'].isna()]['bairro'].unique()
print("Siglas não encontradas:", faltantes)

# Visualizar resultado
df[['bairro', 'bairro_nome', 'rpa']].head(20)

df.info()

output_file = base_dir / "censo_arboreo_padronizado.csv"
df.to_csv(output_file, index=False)
print(f"Arquivo salvo: {output_file}")

"""**5. Conversão e padronização de variáveis temporais**"""

if 'data_monitoramento' in df.columns:
    print("Tipo da coluna data_monitoramento:", df["data_monitoramento"].dtype)
else:
    print("Coluna 'data_monitoramento' nao encontrada no dataset.")
    
if 'data_plantio' in df.columns:
    print("Tipo da coluna data_plantio:", df["data_plantio"].dtype)
else:
    print("Coluna 'data_plantio' nao encontrada no dataset.")

# ============================================
# CONVERSÃO DE VARIÁVEIS DE DATA PARA datetime
# ============================================
import pandas as pd

# converte as colunas para formato de data (YYYY-MM-DD)
if 'data_plantio' in df.columns:
    df["data_plantio"] = pd.to_datetime(df["data_plantio"], errors="coerce", dayfirst=True)
if 'data_monitoramento' in df.columns:
    df["data_monitoramento"] = pd.to_datetime(df["data_monitoramento"], errors="coerce", dayfirst=True)

# checagem de tipos
colunas_data = []
if 'data_plantio' in df.columns:
    colunas_data.append('data_plantio')
if 'data_monitoramento' in df.columns:
    colunas_data.append('data_monitoramento')
    
if colunas_data:
    print(df[colunas_data].info())
    # amostra das 10 primeiras linhas convertidas
    print("\nAmostra de datas convertidas:")
    print(df[colunas_data].head(10))
else:
    print("Nenhuma coluna de data encontrada para converter.")

output_file = base_dir / "censo_arboreo_padronizado_temporal.csv"
df.to_csv(output_file, index=False)
print(f"Arquivo salvo: {output_file}")

"""**6. Tratamento de valores extremos**"""

# ============================================
# TRATAMENTO DE VALORES EXTREMOS — Verdefica
# - Corrige unidade de 'altura' (cm -> m quando > 60)
# - Gera relatório conforme checklist
# ============================================
import pandas as pd
import numpy as np

OK   = "✅"
WARN = "⚠️"

report = []

# 0) Coerção para numérico nas colunas alvo (sem quebrar o DF)
alvos = ["altura", "copa", "dap"]
for c in alvos:
    if c in df.columns and not pd.api.types.is_numeric_dtype(df[c]):
        df[c] = pd.to_numeric(df[c], errors="coerce")

# 1) Estatísticas ANTES (útil para comparar)
def stats(col):
    desc = df[col].describe(percentiles=[.01,.5,.99])
    return float(desc["min"]), float(desc["mean"]), float(desc["max"])

# 2) Correção de unidade em ALTURA: >60 -> assume cm e divide por 100
if "altura" in df.columns and pd.api.types.is_numeric_dtype(df["altura"]):
    report.append("\n(iv) TRATAMENTO DE VALORES EXTREMOS")

    # backup de segurança
    if "altura_original" not in df.columns:
        df["altura_original"] = df["altura"]

    # stats antes
    mn0, mean0, mx0 = stats("altura")
    acima60 = (df["altura"] > 60).sum()
    total_valid = df["altura"].notna().sum()
    pct_acima60 = 100 * acima60 / total_valid if total_valid else 0.0

    report.append(f"{OK} Estatísticas (antes) – altura: min={mn0:.2f}, média={mean0:.2f}, max={mx0:.2f}")
    report.append(f"{OK} Registros > 60 (possível cm): {acima60} de {total_valid} ({pct_acima60:.2f}%)")

    # regra de conversão
    mask_cm = df["altura"] > 60
    df.loc[mask_cm, "altura"] = df.loc[mask_cm, "altura"] / 100.0

    # stats depois
    mn1, mean1, mx1 = stats("altura")
    report.append(f"{OK} Estatísticas (depois) – altura: min={mn1:.2f}, média={mean1:.2f}, max={mx1:.2f}")
    report.append(f"{'✅' if mx1 <= 60 else '⚠️'} Altura com máximo plausível (<= 60 m).")

else:
    report.append("\n(iv) TRATAMENTO DE VALORES EXTREMOS")
    report.append(f"{WARN} Coluna 'altura' ausente ou não numérica.")

# 3) Verificação para outras numéricas de interesse (copa, dap)
cand_nums = [c for c in df.columns if any(k in c.lower() for k in ["altura","copa","dap"])]
if cand_nums:
    for c in cand_nums:
        if pd.api.types.is_numeric_dtype(df[c]):
            mn, mean, mx = stats(c)
            report.append(f"{OK} Estatísticas – {c}: min={mn:.2f}, média={mean:.2f}, max={mx:.2f}")
            if "altura" in c.lower():
                report.append(f"{'✅' if mx<=60 else '⚠️'} Altura com máximo plausível (<=60 m).")
        else:
            report.append(f"{WARN} Coluna '{c}' não é numérica ({df[c].dtype}).")
else:
    report.append(f"{WARN} Não encontrei colunas com 'altura', 'copa' ou 'dap' para verificar extremos.")

# 4) Mostrar relatório
print("\n".join(report))

output_file = base_dir / "censo_arboreo_padronizado_temporal_extremo.csv"
df.to_csv(output_file, index=False)
print(f"Arquivo salvo: {output_file}")

"""**7. Análise de variáveis espaciais**"""

import pandas as pd
import numpy as np

# =============================================
# ANÁLISE DAS VARIÁVEIS ESPACIAIS
# =============================================

espaciais = ["x", "y", "latitude", "longitude", "x_wgs84", "y_wgs84"]

print("=== Tipos e nulos ===")
print(df[espaciais].info())
print("\nValores nulos por coluna:")
print(df[espaciais].isna().sum())

# Estatísticas descritivas
print("\n=== Estatísticas descritivas ===")
print(df[espaciais].describe().T)

# Diferenças entre colunas equivalentes
df["dif_long"] = (df["longitude"] - df["x_wgs84"]).abs()
df["dif_lat"]  = (df["latitude"] - df["y_wgs84"]).abs()

print("\n=== Diferença entre coordenadas duplicadas (longitude/x_wgs84 e latitude/y_wgs84) ===")
print(df[["dif_long", "dif_lat"]].describe())

# Amostra de possíveis inconsistências
print("\n=== Amostras com diferença significativa (>0.0001) ===")
erros = df[(df["dif_long"] > 0.0001) | (df["dif_lat"] > 0.0001)]
print(erros[["longitude", "x_wgs84", "latitude", "y_wgs84"]].head(10))

# Faixas plausíveis de Recife
print("\n=== Faixas esperadas para Recife ===")
print("Longitude: entre -35.05 e -34.80")
print("Latitude : entre  -8.10 e  -7.90")

# Filtra registros fora do Recife (coordenadas incorretas)
fora_recife = df[
    (df["longitude"] < -35.05) | (df["longitude"] > -34.80) |
    (df["latitude"]  <  -8.10) | (df["latitude"]  >  -7.90)
]
print(f"\nRegistros fora da faixa geográfica esperada: {len(fora_recife)}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))

# Mapa em latitude/longitude
plt.subplot(1,2,1)
plt.scatter(df["longitude"], df["latitude"], s=1, alpha=0.3)
plt.title("Coordenadas Geográficas (longitude x latitude)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")

# Mapa em UTM
plt.subplot(1,2,2)
plt.scatter(df["x"], df["y"], s=1, alpha=0.3, color="green")
plt.title("Coordenadas Projetadas (x x y)")
plt.xlabel("x (UTM)")
plt.ylabel("y (UTM)")

plt.tight_layout()
plt.show()

print("Faixas de valores esperadas:")
print("Longitude verdadeira: entre -35.05 e -34.80")
print("Latitude verdadeira : entre  -8.10 e  -7.90\n")

print("Faixas atuais do dataset:")
print("longitude:", df["longitude"].min(), "→", df["longitude"].max())
print("latitude :", df["latitude"].min(), "→", df["latitude"].max())

print("Diferença média entre pares de colunas:")
print("longitude vs x:", (df["longitude"] - df["x"]).abs().mean())
print("latitude  vs y:", (df["latitude"] - df["y"]).abs().mean())
print("longitude vs x_wgs84:", (df["longitude"] - df["x_wgs84"]).abs().mean())
print("latitude  vs y_wgs84:", (df["latitude"] - df["y_wgs84"]).abs().mean())

# Se precisar:
# !pip install pyproj folium

import pandas as pd
from pyproj import Transformer
import folium
from folium.plugins import FastMarkerCluster

# 1) Filtrar RPAs válidas e coordenadas existentes
df_mapa = df[df['rpa'].between(1, 6)].copy()
df_mapa = df_mapa[df_mapa['x'].notna() & df_mapa['y'].notna()]

# 2) Converter de UTM 25S para WGS84 (lon/lat)
# Tente primeiro SIRGAS2000/UTM 25S (EPSG:31985). Se ficar estranho, use WGS84/UTM 25S (EPSG:32725).
transformer = Transformer.from_crs("EPSG:31985", "EPSG:4326", always_xy=True)
# Se necessário, troque para:
# transformer = Transformer.from_crs("EPSG:32725", "EPSG:4326", always_xy=True)

lon, lat = transformer.transform(df_mapa['x'].values, df_mapa['y'].values)
df_mapa['lon'] = lon
df_mapa['lat'] = lat

# Checagem rápida: deve dar ~ (-34.9, -8.0)
df_mapa[['lon','lat']].head()

# 3) Mapa interativo em Recife
center = [df_mapa['lat'].mean(), df_mapa['lon'].mean()]
m = folium.Map(location=center, zoom_start=12, tiles="CartoDB positron")

locs = df_mapa[['lat','lon']].values.tolist()
FastMarkerCluster(locs).add_to(m)

m

colunas_nao_numericas = df.select_dtypes(exclude=['number']).columns
colunas_nao_numericas

df[colunas_nao_numericas].nunique().sort_values(ascending=False)

# Seleciona apenas as colunas não numéricas
colunas_nao_numericas = df.select_dtypes(exclude=['number']).columns

# Remove a coluna 'globalid', se existir
colunas_nao_numericas = [c for c in colunas_nao_numericas if c != 'globalid']

# Mostra os valores únicos de cada coluna, um por linha
for c in colunas_nao_numericas:
    print(f"\n--- {c} ---")
    for valor in df[c].dropna().unique():
        print(valor)

output_file = base_dir / "censo_arboreo_padronizado_temporal_geografico.csv"
df.to_csv(output_file, index=False)
print(f"Arquivo salvo: {output_file}")